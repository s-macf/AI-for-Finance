---
title: 'CS971: AI for Finance Assignment 2'
author: "Stewart Macfarlane, Vladimir Lenkov, Alvee Kabir"
date: "11-04-2025"
output:
  pdf_document: default
---

```{R include=FALSE}
#install.packages("tidyquant")
#install.packages("quantmod")
#install.packages("GA")
#install.packages("rnn")
#install.packages("TTR")
library(tidyquant)
library(quantmod)
library(GA)
library(rnn)
library(TTR)
set.seed(09042025)
```

# Project Background



# Asset Selection

The initial assets were gathered using the S&P 500 index, a stock market index that tracks the performance of 500 of the largest trading companies in the United States. In addition to having an extensive collection of assets, this index represents a wide range of sectors including but not limited to technology, healthcare and finance. This serves as a solid foundation for selecting a significant asset for the project.

```{R echo=TRUE, message=FALSE}
assets <- tq_index("SP500") # Load 500 assets from S&P
head(assets) # Display the 500 assets
```

Furthermore, the daily returns for each asset are retrieved to calculate the Sharpe ratio.

```{R echo=TRUE, message=FALSE}
load_daily_returns <- function(asset_symbols, startDate, endDate) {
  removed_assets <- c()
  
  assets_train <- lapply(asset_symbols, function(sym) {
    tryCatch(
      dailyReturn(getSymbols(sym, from = startDate, to = endDate, auto.assign = FALSE)),
      
      error = function(e) {
        removed_assets <<- append(removed_assets, sym)
        cat("\nSkipping asset:", sym, "\n")
      }
    )
  })
  
  asset_symbols <- setdiff(asset_symbols, removed_assets)
  df <- setNames(do.call(merge, c(assets_train, all = T)), asset_symbols)
  df <- na.omit(df)
  df <- df[, colSums(is.na(df)) < nrow(df)]
  return(df)
}
```

The start and end date for the period to be used to make next-day predictions has been set to two months. This is so that enough data is present to reflect vital patterns to make predictions, however, not a long enough time period whereby the large quantity of historic data will negatively skew results.

```{R}
asset_symbols <- assets$symbol
startDate <- "2024-08-01"; endDate <- "2024-12-31"
df <- load_daily_returns(asset_symbols, startDate, endDate)
```

```{R}
calc_sharpe_ratio <- function(returns, rf_rate) {
  mean_return <- mean(returns)
  risk <- sd(returns)
  sharpe_ratio <- ((mean_return - rf_rate) / risk) * sqrt(252)
  return(sharpe_ratio)
}
```

The performance of all 500 assets is evaluated and compared to one another based on their Sharpe ratios. The Sharpe ratio serves as a valuable tool for measuring investment prospects for a specific asset as it enables the comparison of the expected return for the level of risk being taken (risk-adjusted return). In this case, a risk-free rate is dynamically retrieved and used within the Sharpe ratio calculation for each asset.

$$ S_a = \frac{E[R_a - R_b]}{\sigma_a}$$
$$Where: \newline S_a = \text{Sharpe Ratio }  \newline E = \text{Expected Return } $$
$$R_a = \text{Asset Return } \newline R_b = \text{Risk Free Rate } \newline \sigma_a = \text{Asset Risk}$$

```{R}
rf_rate <- as.numeric(last(getSymbols("DGS3MO", src = "FRED", auto.assign = FALSE)))/100 /252
best_res <- calc_sharpe_ratio(df[, 1], rf_rate)
best_asset <- NULL
for (col in colnames(df)) {
  curr_sharpe <- calc_sharpe_ratio(df[, col], rf_rate)
  if (curr_sharpe > best_res) {
    best_res <- curr_sharpe
    best_asset <- col
  }
}
```

```{R include=FALSE}
best_asset
```

Once all assets have been compared, the best-performing asset is selected to be used to make next-day predictions in alignment with a comprehensive trading rule. All relevant data is then retrieved, this includes opening, high, low and closing prices.

```{R}
best_asset_data <- getSymbols(best_asset, from = startDate, to = endDate, auto.assign = FALSE)
```

# Data Preprocessing

```{R}
rsi = TTR::RSI(Cl(best_asset_data), n = 14)
ema_short = TTR::EMA(Cl(best_asset_data), n = 12)
ema_long = TTR::EMA(Cl(best_asset_data), n = 26)
macd = ema_short - ema_long
volume_ma = TTR::SMA(Vo(best_asset_data), n = 20)
```

```{R}
best_asset_data$RSI = rsi
best_asset_data$MACD = macd
best_asset_data$Volume_MA = volume_ma
best_asset_data = na.omit(best_asset_data)
```

```{R include=FALSE}
#best_asset_data
```

```{R}
data <- data.frame(best_asset_data[,1], best_asset_data[,2], best_asset_data[,3], best_asset_data[,4], best_asset_data[,5], best_asset_data[,6], best_asset_data[,7], best_asset_data[,8], best_asset_data[,9])
min_max_normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

data_scaled <- as.data.frame(lapply(data, min_max_normalize))
```

```{R include=FALSE}
#data_scaled
```

```{R}
train_test_split <- function(asset, seq_length, target_feature, test_size = 0.2) {
  asset_matrix <- as.matrix(asset)
  num_seq <- nrow(asset_matrix) - seq_length + 1
  num_features <- ncol(asset_matrix)
  
  seq_data <- array(dim = c(num_seq, seq_length, num_features))
  
  for (index in 1:(nrow(asset_matrix) - seq_length +1)) {
    seq_data[index, , ] <- asset_matrix[index:(index + seq_length - 1), ]
  }
  
  test_set_size <- round(test_size * nrow(seq_data))
  train_set_size <- nrow(seq_data) - test_set_size
  
  x_train <- seq_data[1:train_set_size, 1:(seq_length - 1), , drop = FALSE]
  y_train <- seq_data[1:train_set_size, seq_length, target_feature, drop = FALSE]
  
  x_test  <- seq_data[(train_set_size + 1):nrow(seq_data), 1:(seq_length - 1), , drop = FALSE]
  y_test  <- seq_data[(train_set_size + 1):nrow(seq_data), seq_length, target_feature, drop = FALSE]
  
  return(list(x_train = x_train,
              y_train = y_train,
              x_test = x_test,
              y_test = y_test))
}
```

```{R}
seq_length <- 8
open <- paste(best_asset, "Open", sep = ".")
high <- paste(best_asset, "High", sep = ".")
low <- paste(best_asset, "Low", sep = ".")
close <- paste(best_asset, "Close", sep = ".")
rsi = "RSI"
macd = "MACD"
volume_ma = "Volume_MA"
features <- data_scaled[, c(open, high, low, close, rsi, macd, volume_ma)]

split_data <- train_test_split(features, seq_length, ncol(features))
x_train <- split_data$x_train
y_train <- split_data$y_train
x_test <- split_data$x_test
y_test <- split_data$y_test
```

```{R include=FALSE}
str(features)
```

```{R}
# For hyperparameter tuning, we split part of x_train/y_train to act as a validation set
# For example, we use 80% for training and 20% for validation
split_validation <- function(x, y, valid_prop = 0.2) {
  total <- dim(x)[1]                    
  valid_size <- round(valid_prop * total)
  train_size <- total - valid_size
  
  # Subset x without dropping dimensions
  x_train_tune <- x[1:train_size, , , drop = FALSE]
  x_val <- x[(train_size + 1):total, , , drop = FALSE]
  
  # Force y to be a matrix to ensure two dimensions
  y <- as.matrix(y)
  
  y_train_tune <- y[1:train_size, , drop = FALSE]
  y_val <- y[(train_size + 1):total, , drop = FALSE]
  
  return(list(
    x_train_tune = x_train_tune,
    y_train_tune = y_train_tune,
    x_val = x_val,
    y_val = y_val
  ))
}


# Split the training data for tuning 
split_data <- split_validation(x_train, y_train, valid_prop = 0.2)
x_train_tune <- split_data$x_train_tune
y_train_tune <- split_data$y_train_tune
x_val <- split_data$x_val
y_val <- split_data$y_val
```

# Optimising LSTM Parameters

The LSTM parameters are optimised using two techniques: grid search and genetic algorithms. This was done to compare the results from utilising traditional versus evolutionary approaches and conclude the pros and cons of each. Furthermore, the optimised parameters identified from this process are used by the LSTM to make predictions in conjunction with the proposed trading rule.

```{R echo=TRUE, message=FALSE}
# Define a tuning function that trains the LSTM and returns the mean squared error on the validation set
tune_lstm <- function(learningrate, hidden_dim, num_layers, numepochs, batch_size) {
  model <- trainr(
    Y = y_train_tune,
    X = x_train_tune,
    learningrate = learningrate,
    hidden_dim = hidden_dim,
    num_layers = num_layers,
    numepochs = numepochs,
    network_type = "lstm",
    seq_to_seq_unsync = TRUE,
    batch_size = batch_size
  )
  # Generate predictions on the validation set
  predictions <- predictr(model, x_val)
  mse <- mean((predictions - y_val)^2, na.rm = TRUE)
  return(mse)
}
```

## Grid Search

Grid search is a traditional approach to identifying optimal hyperparameter values for machine learning models. In this approach, the key hyperparameters to be tested are listed inside a vector, which the algorithm then systematically iterates over each combination and records the result. In this case, the mean squared error (MSE) is used on validation data to determine the current performance.

```{R echo=TRUE, message=FALSE}
# Set up grid search parameters (you can adjust or expand the grid as needed)
learningrate_vals <- c(0.001, 0.005, 0.01)
hidden_dim_vals <- c(8, 16, 32, 64, 128)
num_layers_vals <- c(1, 2, 3)
numepochs_vals <- c(50, 100, 150, 200)
batch_size_vals <- c(8, 16, 32, 64)

# Initialize a data frame to store results
results <- data.frame(
  learningrate = numeric(0),
  hidden_dim = numeric(0),
  num_layers = numeric(0),
  numepochs = numeric(0),
  batch_size = numeric(0),
  mse = numeric(0)
)
```

```{R echo=TRUE, message=FALSE}
# Grid search
for (lr in learningrate_vals) {
  for (hd in hidden_dim_vals) {
    for (nl in num_layers_vals) {
      for (ne in numepochs_vals) {
        for (bs in batch_size_vals) {
          current_mse <- tune_lstm(learningrate = lr,
                                   hidden_dim = hd,
                                   num_layers = nl,
                                   numepochs = ne,
                                   batch_size = bs)
          results <- rbind(results, data.frame(
            learningrate = lr,
            hidden_dim = hd,
            num_layers = nl,
            numepochs = ne,
            batch_size = bs,
            mse = current_mse
          ))
          #cat("Tested: lr=", lr, ", hd=", hd, ", nl=", nl, ", ne=", ne, ", bs=", bs, 
          #    "-> MSE=", current_mse, "\n")
        }
      }
    }
  }
}

best_params_GS <- results[which.min(results$mse), ]
```

```{R include=FALSE}
# Identify the best parameter set (lowest MSE)
cat("Best Hyperparameters:\n")
print(best_params_GS)
```

## Genetic Algorithm

A genetic algorithm is an evolutionary process that mimics natural selection and genetics. This algorithm has been used to identify optimal hyperparameters within specified ranges (lower and upper). This implementation has a maximum of 100 iterations and will stop executing if the fitness does not improve after 20 iterations. The fitness is determined using the fitness function which evaluates performance against the MSE value.

```{R echo=TRUE, message=FALSE}
fitness_function <- function(params) {
  learningrate <- params[1]
  hidden_dim <- round(params[2])
  num_layers <- round(params[3])
  numepochs <- round(params[4])
  batch_size <- round(params[5])
  
  mse <- tune_lstm(
    learningrate = learningrate,
    hidden_dim = hidden_dim,
    num_layers = num_layers,
    numepochs = numepochs,
    batch_size = batch_size
  )
  return(-mse)
}

ga_result <- ga(
  type = "real-valued",
  fitness = fitness_function,
  lower = c(0.0001, 8, 1, 50, 8),
  upper = c(0.01, 128, 3, 200, 64),  
  popSize = 20, 
  maxiter = 100,  
  run = 20  
)

best_params_GA <- ga_result@solution
```

```{R include=FALSE}
cat("Best Hyperparameters:\n")
print(best_params_GA)
```

### Optimisation Comparisons

Through experimenting with both of the above approaches key benefits and downfalls of each have been identified. First, Grid search is strictly limited to searching the specified hyperparameters whereas the GA solution can navigate the search space more effectively only being restricted to lower and upper bounds. Furthermore, both algorithms are computationally expensive, although, genetic algorithms have an edge as they can effectively terminate execution if the performance has not improved over a specified number of iterations, whereas grid search must evaluate all combinations. Finally, this difference between the two approaches is what sets them apart as a GA can get stuck in a local maximum and never converge to the optimal solution, on the other hand, grid search will evaluate all provided combinations guaranteeing the most optimal from the provided is found.

# LSTM

```{R echo=TRUE, message=FALSE}
train_lstm <- function(params){
  model <- trainr(
    Y = y_train,
    X = x_train,
    learningrate = as.numeric(params[1]),
    hidden_dim = as.numeric(round(params[2])),
    num_layers = as.numeric(round(params[3])),
    numepochs = as.numeric(round(params[4])),
    bactch_size = as.numeric(round(params[5]))
    network_type = "lstm",
    activation = "tanh",
    seq_to_seq_unsync = T,
  )
  return(model)
}
```

```{R echo=TRUE, message=FALSE}
lstm_GS <- train_lstm(best_params_GS)
lstm_GA <- train_lstm(best_params_GA)
```

```{R include=FALSE}
draw_graph <- function(predictions, actual, msg, legend_pos = "bottomright") {
  ylim_range <- range(c(actual, predictions))
  plot(actual, type = "l", col = "blue", lwd = 2, main = msg,
       xlab = "Time Steps", ylab = "Values", ylim = ylim_range)
  lines(predictions, col = "red", lwd = 2)
  legend(legend_pos, legend = c("Actual", "Predicted"), col = c("blue", "red"), lty = 1, lwd = 2)
}
```

```{R include=FALSE}
draw_two_graph <- function(predictions1, actual1, predictions2, actual2, msg1, msg2, overall_title) {
  par(mfrow = c(1, 2), oma = c(0, 0, 2, 0))
  
  # First graph
  draw_graph(predictions1, actual1, msg1)
  
  # Second graph
  draw_graph(predictions2, actual2, msg2)
  mtext(overall_title, outer = TRUE, cex = 1.5)
}
```

```{R fig.width=12, fig.height=4, echo=FALSE}
predictions1_GS <- predictr(lstm_GS, x_train)
predictions2_GS <- predictr(lstm_GS, x_test)
draw_two_graph(predictions1_GS, y_train, predictions2_GS, y_test, "Train Data", "Test Data", "Grid Search Optimised Paramaters")
```

```{R fig.width=12, fig.height=4, echo=FALSE}
predictions1_GA <- predictr(lstm_GA, x_train)
predictions2_GA <- predictr(lstm_GA, x_test)
draw_two_graph(predictions1_GA, y_train, predictions2_GA, y_test, "Train Data", "Test Data", "Genetic Algorithm Optimised Paramaters")
```

```{r}
starting_funds = 10000
investment = starting_funds
shares = 0

inverse_scale <- function(scaled_value, unscaled_min, unscaled_max) {
  scaled_value * (unscaled_max - unscaled_min) + unscaled_min
}

predictions_scaled = predictr(lstm_GS, x_test)
unscaled_min_close = min(data[, paste(best_asset, "Close", sep = ".")])
unscaled_max_close = max(data[, paste(best_asset, "Close", sep = ".")])

predictions_unscaled = inverse_scale(predictions_scaled, unscaled_min_close, unscaled_max_close)
actual_unscaled = inverse_scale(y_test, unscaled_min_close, unscaled_max_close)
#predictions_unscaled
#actual_unscaled
```

```{r, based on closing}
trading_rule = data.frame(
  Date = index(tail(best_asset_data, nrow(y_test))),
  actual_price = rep(NA, nrow(y_test)),
  predicted_price = rep(NA, nrow(y_test)),
  action = character(nrow(y_test)),
  asset_value = numeric(nrow(y_test)),
  shares_held = numeric(nrow(y_test))
)

trading_rule$asset_value[1] = investment
trading_rule$shares_held[1] = shares
trading_rule$actual_price = actual_unscaled
trading_rule$predicted_price = predictions_unscaled

threshold_buy = 0.05
threshold_sell = -0.05

for(i in 1:nrow(trading_rule)){
  if(i>1){
    investment = trading_rule$asset_value[i-1]
    shares = trading_rule$shares_held[i-1]
  }
  current_price = trading_rule$actual_price[i]
  predicted_price = trading_rule$predicted_price[i]
  action = "HOLD"
  
  if(!is.na(predicted_price) && !is.na(current_price)){
    predicted_change_percentage = (predicted_price - current_price) / current_price
    if (predicted_change_percentage > threshold_buy && investment > 0) {
      action = "BUY"
      buy_quantity = floor(investment / current_price)
      shares = shares + buy_quantity
      investment = investment - (buy_quantity * current_price)
    } else if (predicted_change_percentage < threshold_sell && shares > 0) {
      action = "SELL"
      sell_value = shares * current_price
      investment = investment + sell_value
      shares = 0
    }
  }
  
  trading_rule$action[i] = action
  trading_rule$asset_value[i] = investment + (shares * current_price)
  trading_rule$shares_held[i] = shares
}
```

```{r, based on previous day}
trading_rule = data.frame(
  Date = index(tail(best_asset_data, nrow(y_test))),
  actual_price = rep(NA, nrow(y_test)),
  predicted_price = rep(NA, nrow(y_test)),
  action = character(nrow(y_test)),
  asset_value = numeric(nrow(y_test)),
  shares_held = numeric(nrow(y_test))
)

trading_rule$asset_value[1] = investment
trading_rule$shares_held[1] = shares
trading_rule$actual_price = actual_unscaled
trading_rule$predicted_price = predictions_unscaled

threshold_buy = 0.01
threshold_sell = -0.01

next_day_action = character(nrow(trading_rule))
next_day_action[1] = "HOLD"

for(i in 1:(nrow(trading_rule) - 1)){
  current_price = trading_rule$actual_price[i]
  predicted_price = trading_rule$predicted_price[i]
  action = "HOLD"

  if(!is.na(predicted_price) && !is.na(current_price)){
    predicted_change_percentage = (predicted_price - current_price) / current_price
    if(predicted_change_percentage > threshold_buy){
      action = "BUY"
    } else if(predicted_change_percentage < threshold_sell){
      action = "SELL"
    } else if(predicted_change_percentage < threshold_buy && predicted_change_percentage > threshold_sell){
      action = "HOLD"
    }
  }
  next_day_action[i + 1] = action
}

for(i in 1:nrow(trading_rule)){
  if(i > 1){
    investment = trading_rule$asset_value[i-1]
    shares = trading_rule$shares_held[i-1]
  }

  trade_action = next_day_action[i]
  current_price = trading_rule$actual_price[i]

  if(trade_action == "BUY" && investment > 0){
    buy_quantity = floor(investment / current_price)
    shares = shares + buy_quantity
    investment = investment - (buy_quantity * current_price)
  } else if(trade_action == "SELL" && shares > 0){
    sell_value = shares * current_price
    investment = investment + sell_value
    shares = 0
  }

  trading_rule$action[i] = trade_action
  trading_rule$asset_value[i] = investment + (shares * current_price)
  trading_rule$shares_held[i] = shares
}
```

```{r}
final_asset_value = tail(trading_rule$asset_value, 1)
initial_investment = starting_funds
profit_loss = final_asset_value - initial_investment
roi = (profit_loss / initial_investment) * 100

cat("\nFinal Asset Value: $", round(final_asset_value, 2), "\n")
cat("Profit/Loss: $", round(profit_loss, 2), "\n")
cat("Return on Investment (ROI): ", round(roi, 2), "%\n")

plot_trading_simulation = function(trade_log) {
  plot(trading_rule$Date, trading_rule$asset_value, type = "l", col = "green",
       xlab = "Date", ylab = "Asset Value ($)",
       main = "Trading Strategy Performance")
  grid()
}

plot_trading_simulation(trade_log)
```

```{r}
print(trading_rule)
```